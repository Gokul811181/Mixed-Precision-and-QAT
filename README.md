# Mixed-Precision-and-QAT
Mixed Precision Training and Quantization Aware Training (QAT) are techniques that optimize deep learning models to improve computational efficiency and reduce memory usage without significant loss in accuracy. Below, we go through the step-by-step process for implementing both .Mixed Precision Training (MPT)

Benefits of Mixed Precision Training:
->Speeds up training (2-3x on modern GPUs).
->Reduces memory consumption.
->Helps train larger models with the same hardware.

Benefits of Quantization Aware Training (QAT):
->Reduces model size (up to 75%).
->Improves inference speed (2-4x on CPUs).
->Maintains accuracy close to FP32.
